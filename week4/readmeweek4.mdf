::::::::::::::::::::marzo/12/2018::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
STRUCTURE AND OPTIMIZATION:

Diferencias entre:
 1- Hacer Join primero de todos un set de datos completos (clientes y facturas) para luego filtrar lo que necesitamos
 2- Hacer el filter de clientes (lo que necesitamos), luego el filter requerido para facturas ; fnalmente join (mejor PERFORMANCE!!!)
 3- Hacer el producto cartesiano

pERO Será que existe alguna forma en que al escribir cod con base al punto 3, spark es capaz de cambiar internamente al 2 y obtener el mismo tiempo??
R/ SI!! DANDO un poco de STRUCTURal informatio to Spark Y de una spark realizará optimizaciones por nosotros :) :) :)  


::::::::::::::STRUCTURED vs UNSTRUCTURED::::::::::::::::::::::::::::::::::::::::::

tOdos los datos tiene como el ciclo de ir de lo  UNSTRUCTURED a lo STRUCTURED PASANDO POR SEMI-STRUCTURED, es decir 

UNSTRUCTURED(logs, images) >> SEMI-STRUCTURED(json, xml que son esquemas no tan rigisos como "database tables")  >>  STRUCTURED(database tables)

HASTA LO QUE HEMOS VISTO solo llegabamos a SEMI-STRUCTURED con el uso de RDD, es decir que no sabe nada de datos estructurados es decir sabe la clase pero no el detalle
sabe que es una persona, pero no como está conformado (que atributos??)
Ejemplo sabe que es un RDDÑ[Account  pero no sabe que en su interior hay un id etc y que por x columna se logra mayor performance etc (como en las bases de datos)

Al estructurarlo, por ejemplo usando Hive para crear la base de datos entonces Spark va a saber como optimizar y que distribuir 

En una base de datos/Hive
Declaramos transformaciones
Indicamos operaciones predefinidas 

con lo anterior viene SPARK - SQL que ofrece el estructurado de la información y optimiza varias cosas por nosotros, así el hecho de que nos toque mejorar el performance por nuestra cuenta disminuye 

::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

::::::::::::::::::::marzo/14/2018::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
SPARK SQL, lo podemos ver como una librería encima de Spark :) que nos permite lo siguiente:

1- sOPORTA EL procesamiento Relacional y accediendo vía sql pero por debajo es FUNCIONAL(maps and filter operations).



Una tabla representa un coleccion de objectos tipo Cliente, usuario etc

Un Relation no es mas de una Tabla
Attributes son columnas 
Rows son records of Tuples


DataFrames es una abstraccion de Spark-Sql
conceptualmente como una tabla en bases de datos relacionales o RDDs con un schema conocido

DataFrames son untype, esto es que el compiler no verifica el tipo de su schema, es decir a diferfencia de RDD[T que si tiene un type eje RDD de Customers , dataframe no tiene


Tarnsformations on DataFrames tambien se conocen como untype Transformations

Todo inicia con SparkSession 

Cómo crear un DataFrame:
1- Desde un Rdd
2 Desde un archivo




::::LITERALS:::::
createOrReplaceTemView 

Los Sql statements disponible son los que HiveQl soporte 

::::::::::::::::::::DATAFRAMES PARTE 1:::::::::::::::::::::::::::::::::::::








Structs:
Es un contenedor de posibles Struct fields de diferentes tipos, lo usan para representar clases (case clases) como data Types en Base de datos!! (ver min 6:12)


Revisando los datos (operaciones para visualizar el contenido de un DataFrame)
show(): Muestra los datos de manera tabular --Los primeros 20 
printSchema(): Muestra el schema del DataFrame al que le aplico esta operacion,  en estructura de arbol


::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::


::::::::::::::::::::TRANSFORMATIONS:::::::::::::::::::::::

Select: Para seleccinar solo x columnas y retornar otro Dataframe(parecido al Map )
agg: Aggregate para retornar un valor como resultado de x operación (lo relaciono con el reduce , aggregate, )
GrouppBy: Agrupa el dataFrame basado en X columna.
join: Inner Join con otro DataFrame.
filer
limit
orderBy
where 
as
sort
union
drop
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::
TRABAJANDO CON COLUMNAS

ENVOLVIENDOLO en df("age") > 30 o ($"age" > 30) o ("age > 30")

:::::::::::::::::::::::::::::::::::::::::::::::::::::::::

Group and Aggregate DataFrames::::::::::
Es un PATRON común que siempre se haga primero el group y luego aggregate , cuando estamos trabajando con DataFrames

groupBy: Retorna un tipo RelationalGroupedDataSet
aggregate: Aplica una operacion aggregate al RelationalGroupedDataSet anterior, count sum , max , min avg  (ver min 22:15)


min 26:23 están las referencias para ver todas las operaciones sobre RelationalGroupedDataSet


::::::::::::::::::::DATAFRAMES PARTE 2:::::::::::::::::::::::::::::::::::::
Cleaning Data with DataFrames:
ante valores nulos or NaN que hacemos? drop de esos valores o replace por una constante?
 Spark ofrece drop en sus variedades como , ,
 drop() : para any
drop(all) todas las columnas
drop(Array(column1m column2)): solo esas columnas

Y TAMBIEN OFRECE:

Replace unwanted values:

fill(0): todo lo que sea nulo  y numerico lo reemplaza por el valor dado, en éste caso 0.
fill(map("salario"-> 0)): solo los valore nulos en esta columna se reemplaza
fill(Array("codigo"), map("12"-> "3000")); Reemplazala columna (en éste caso codigo) por 


::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

::::::::::::::::::::ACTIONS:::::::::::::::::::::::
collect()
count()
first()
show()
take()




::::::::::::::::::::::::::::::::::::::::::::::::::

JOINS:
IGUAL QUE RDDS adicionando el :
leftsemi: 
A CADA JoIN LE debemos decir por cuales columnas usar el join eje
idaf1.join(df2, $"df1.c1" === $"df2.c2")

para cambiar el tipo de join, entonces el método recibe otro parámetro:
idaf1.join(df2, $"df1.c1" === $"df2.c2", "leftOuterJoin")


::::::::::::::::::::OPTIMIZATIONS:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
Existen 2 compoementes especializados en el Back-end
1) Catalyst : Query Optimizer: compila spark sql programs a RDDs
Catalyst hace posible hacer optimizaciones como:
- Reordering operations
- pODEMOS mover menos datos en la Red porque como son datos estructurados entonces podemos seleccionar la información que interviene en el computo, con RDDS tenemos el objeto completo
- Quitando particionamientos innecesarios 

2) Tungsten: Off-heap Serializer
 Tungsten provee:
- high specialized data encoders: Tomar un esquema y montarlo en memoria lo que significa VELOCIDAD AL MOMENTO DE
 SERIALIZAR Y DESERALIZAR
- Columns based: rEALIZAR operaciones en columnas específicas

Off-heap: Estar libres de la sobrecotsto del Colector de basura o garbage collector 



:::::::::LIMITACIONES DE LOS DATAFRAMES:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
Untypes!!
Lo que significa que los errores aparecerían en tiempo de ejecución JEMPLO  al hacer un select sobre algo que no existye 
LO IDEAL ES QUE sea en tiempo de compilación 

lYMITED DATA TYPE
pUEDA que se nos complique a la hora de represnetan yn tipo que no tengamos en sprk

Sí tus datos no tiene una estructura entonces toca por el lado de RDDs

::::::::::::::::::::DATASETS:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

Cuando le hacemos collect a un DataFrame, éste nos devuelve un Arra[or.papache.spark.sql.Row  (y todo porque DataFrames son Untypes!!)
y para acceder a los datos de éste nos tocaría hacerle un map y castear cada columna del Row al tipo que necesitamos MIN 3:43 MUESTRA LO ENGORROSO

EXISTE ALGO MEJOR? R/ SI!! los DATASETS
uN Dataframe es n DataSet de tipo Row :
Dataframe = DataSet[Row]

Optienes la solucion al tipado y mayor opciones de optimización que usando RDDs (pregunta y mejor performance que Dataframe?? R/ yo diría que igual porque finalmente es el tipo de DataFrame)

DataSet unifica DataFrames y RDDS (conocido como Mix and Match!, esto es poder hacer un groupBy con lambdas como en RDD y liego un average como en DataFrames min 6:32)
en la misma línea o pipe podemos tener rasgos de ambas APIs
 min 9_20
myDataFrame.toDs para obtener el DS
o desde un Json 
O desde Un RDD
o desde un List scala 

y la forma de tipar una columna es $columName.as[]

Comon type transformation on DataSet : Min 13_42 map flatMap filter distinct groupByKey,
 coalsece: 
repartition:


Al agrupar en Data Set el objeto retornado es un keyGrouped.. y a ése se le pueden aplicar operaciones de avg, count etc 
ademas se ñe puede aplicar estas nuevas:
reduceGroup
agg (aggregation): Se le pasa el avg y otras Investigar
mapGroup
flatMapGroup

Un reduceByKey en RDD pasa a ser un GroupByKey seguido de mapGroups en DataSet

