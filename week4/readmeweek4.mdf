::::::::::::::::::::marzo/12/2018::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
STRUCTURE AND OPTIMIZATION:

Diferencias entre:
 1- Hacer Join primero de todos un set de datos completos (clientes y facturas) para luego filtrar lo que necesitamos
 2- Hacer el filter de clientes (lo que necesitamos), luego el filter requerido para facturas ; fnalmente join (mejor PERFORMANCE!!!)
 3- Hacer el producto cartesiano

pERO Será que existe alguna forma en que al escribir cod con base al punto 3, spark es capaz de cambiar internamente al 2 y obtener el mismo tiempo??
R/ SI!! DANDO un poco de STRUCTURal informatio to Spark Y de una spark realizará optimizaciones por nosotros :) :) :)  


::::::::::::::STRUCTURED vs UNSTRUCTURED::::::::::::::::::::::::::::::::::::::::::

tOdos los datos tiene como el ciclo de ir de lo  UNSTRUCTURED a lo STRUCTURED PASANDO POR SEMI-STRUCTURED, es decir 

UNSTRUCTURED(logs, images) >> SEMI-STRUCTURED(json, xml que son esquemas no tan rigisos como "database tables")  >>  STRUCTURED(database tables)

HASTA LO QUE HEMOS VISTO solo llegabamos a SEMI-STRUCTURED con el uso de RDD, es decir que no sabe nada de datos estructurados es decir sabe la clase pero no el detalle
sabe que es una persona, pero no como está conformado (que atributos??)
Ejemplo sabe que es un RDDÑ[Account  pero no sabe que en su interior hay un id etc y que por x columna se logra mayor performance etc (como en las bases de datos)

Al estructurarlo, por ejemplo usando Hive para crear la base de datos entonces Spark va a saber como optimizar y que distribuir 

En una base de datos/Hive
Declaramos transformaciones
Indicamos operaciones predefinidas 

con lo anterior viene SPARK - SQL que ofrece el estructurado de la información y optimiza varias cosas por nosotros, así el hecho de que nos toque mejorar el performance por nuestra cuenta disminuye 

::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

::::::::::::::::::::marzo/14/2018::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
SPARK SQL, lo podemos ver como una librería encima de Spark :) que nos permite lo siguiente:

1- sOPORTA EL procesamiento Relacional y accediendo vía sql pero por debajo es FUNCIONAL(maps and filter operations).



Una tabla representa un coleccion de objectos tipo Cliente, usuario etc

Un Relation no es mas de una Tabla
Attributes son columnas 
Rows son records of Tuples


DataFrames es una abstraccion de Spark-Sql
conceptualmente como una tabla en bases de datos relacionales o RDDs con un schema conocido

DataFrames son untype, esto es que el compiler no verifica el tipo de su schema, es decir a diferfencia de RDD[T que si tiene un type eje RDD de Customers , dataframe no tiene


Tarnsformations on DataFrames tambien se conocen como untype Transformations

Todo inicia con SparkSession 

Cómo crear un DataFrame:
1- Desde un Rdd
2 Desde un archivo




::::LITERALS:::::
createOrReplaceTemView 

Los Sql statements disponible son los que HiveQl soporte 

::::::::::::::::::::DATAFRAMES PARTE 1:::::::::::::::::::::::::::::::::::::








Structs:
Es un contenedor de posibles Struct fields de diferentes tipos, lo usan para representar clases (case clases) como data Types en Base de datos!! (ver min 6:12)


Revisando los datos (operaciones para visualizar el contenido de un DataFrame)
show(): Muestra los datos de manera tabular --Los primeros 20 
printSchema(): Muestra el schema del DataFrame al que le aplico esta operacion,  en estructura de arbol


::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::


::::::::::::::::::::TRANSFORMATIONS:::::::::::::::::::::::

Select: Para seleccinar solo x columnas y retornar otro Dataframe(parecido al Map )
agg: Aggregate para retornar un valor como resultado de x operación (lo relaciono con el reduce , aggregate, )
GrouppBy: Agrupa el dataFrame basado en X columna.
join: Inner Join con otro DataFrame.
filer
limit
orderBy
where 
as
sort
union
drop
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::
TRABAJANDO CON COLUMNAS

ENVOLVIENDOLO en df("age") > 30 o ($"age" > 30) o ("age > 30")

:::::::::::::::::::::::::::::::::::::::::::::::::::::::::

Group and Aggregate DataFrames::::::::::
Es un PATRON común que siempre se haga primero el group y luego aggregate , cuando estamos trabajando con DataFrames

groupBy: Retorna un tipo RelationalGroupedDataSet
aggregate: Aplica una operacion aggregate al RelationalGroupedDataSet anterior, count sum , max , min avg  (ver min 22:15)


min 26:23 están las referencias para ver todas las operaciones sobre RelationalGroupedDataSet


::::::::::::::::::::DATAFRAMES PARTE 2:::::::::::::::::::::::::::::::::::::
Cleaning Data with DataFrames:
ante valores nulos or NaN que hacemos? drop de esos valores o replace por una constante?
 Spark ofrece drop en sus variedades como , ,
 drop() : para any
drop(all) todas las columnas
drop(Array(column1m column2)): solo esas columnas

Y TAMBIEN OFRECE:

Replace unwanted values:

fill(0): todo lo que sea nulo  y numerico lo reemplaza por el valor dado, en éste caso 0.
fill(map("salario"-> 0)): solo los valore nulos en esta columna se reemplaza
fill(Array("codigo"), map("12"-> "3000")); Reemplazala columna (en éste caso codigo) por 


::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

::::::::::::::::::::ACTIONS:::::::::::::::::::::::
collect()
count()
first()
show()
take()




::::::::::::::::::::::::::::::::::::::::::::::::::

JOINS:
IGUAL QUE RDDS adicionando el :
leftsemi: 
A CADA JPIN LE debemos decir por cuales columnas usar el join eje
idaf1.join(df2, $"df1.c1" === $"df2.c2")

para cambiar el tipo de join:
idaf1.join(df2, $"df1.c1" === $"df2.c2", "leftOuterJoin")

