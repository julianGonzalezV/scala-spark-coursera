Driver programs access Spark through a SparkContext object, which represents a
connection to a computing cluster. In the shell, a SparkContext is automatically


The nodes are called Executors.
:::::::::::::
Lo importante de diferenciar el Tranformation del Action es que el Transformation retorna otro RDD Y es una operación LAZY, es decir que no se ejecuta inmediatamente se defina y si en cadena varios LAzy operations y al final se APLICA 
un ACTION(no resuelve a RDD, it´s a result to the DRIVER PROGRAMor write data to an external storage system.) (EJEMPLO "first")entonces viene lo INTERESANTE, spark para porque encuentra el primero. Al no se Lazy los transformatio entonces imagine el desperdicio de memoria y el performance si por cada tranformer OP cargara en 
memoria el resultado.
:::::::::::::
Spark’s RDDs are by default recomputed each time you run an action on
them. If you would like to reuse an RDD in multiple actions, you can ask Spark to
persist it using RDD.persist()

OJO: para la nota anterios persist() es lo mismo que llamar cache()


::::::::::
parallelize() es otra forma de crear RDDs: Esta operacion para cuando uno está aprendiendo está bien pero en un ambiente productivo no es aconsejable porque va a requerir de deamsiada memoria para ello, PARA TEMAS DE TDD SI PUEDE APLICAR PORQUE NOS PRUEBAS DE UN SET DE REGISTROS PEQUEÑOS



::::::::::lineage graph::::::::::
Pensemos en este termino como el hecho de que Spark guarda registro o traza de las dependencias generadas entre los diferendes RDDs creados, 
ejemplo claro es el de hacer dos filter sobre un mismo RDD inicial y al final unirlos, cada filter generó un RDD nuevo pero Spark sabe de donde viene cada uno y como unirlos con UNION.

:::::::::::::::::
collect() shouldn’t be used on large datasets, BE sure that the amount of data is short or at last for your machine: EN la practica casi que no se usa por la misma razón


::::::::::::::::Passing Functions to Spark:::::::::::::::::::::::::::::

Muy importante saber esto ya que hay algo particularcon spark o más bien llamando a spark (mediante map, flatMap, filter....) que consiste en que 
si hacemos referencia a una funcion en la clase actual (y no mediate el uso de lambdas) usando o no la palabra this.xx , entonces se va a enviar a los
executors o nodos de spark toooodo el objeto completo que contiene la función y puede generar errores o problemas de comunicación una vez el objeto sea muy grande 

class SearchFunctions(val query: String) {
	def isMatch(s: String): Boolean = {
		s.contains(query)
	}
	def getMatchesFunctionReference(rdd: RDD[String]): RDD[String] = {
		// Problem: "isMatch" means "this.isMatch", so we pass all of "this"
		rdd.map(isMatch)
	}
	def getMatchesFieldReference(rdd: RDD[String]): RDD[String] = {
		// Problem: "query" means "this.query", so we pass all of "this"
		rdd.map(x => x.split(query))
	}
	def getMatchesNoReference(rdd: RDD[String]): RDD[String] = {
		// Safe: extract just the field we need into a local variable
		val query_ = this.query
		rdd.map(x => x.split(query_))
	}
}

en esos casos es mejor tener val locales en la función y pasarle a spark solo ése elemento para que el no interprete que se le pas todo el objeto.

:::::::::::::::Common Transformations:::::::::::::::
map:::: Recordar que arranca en una categoría y termina en la misma eje de RDD  ----> RDD
y puede ser de un tipo X a uno Y o tipos iguales  RDD[String]  ----> RDD[Boolean]    ......   RDD[String]  ----> RDD[String]  
Por ejemplo que de una URL saquemos el nombre de la compañía.

flatMap:::: Algunas veces vamos de RDD[String]  ----> RDD[Lis(String)]
val lines = sc.parallelize(List("hello world", "hi"))
val words = lines.flatMap(line => line.split(" "))
words.first() // returns "hello"

instead of ending up with an RDD of lists we have an RDD of the elements in those lists!!!!!!!!!!!!!!



::::::::::::matematical set operations:
union
distinct
intersection
subtract
cartesian(otherRDD):  
> cartesiado entr dos RDDs diferentes (pero de mismo tipo) Muy usado para identificar similitudes entre todos los posibles pares, como cada expectativa de un usuario en una oferta 
> O un cartesiano de un RDD con el mismo para ver similitudes entre sus registros
OJO PARA LA OPERACIONES ENTRE DOS RDDs ES necesarioq ue ambos sean del mismo tipo, es decir 2 RDDs de [String] [Boolean] etc peero no entre tipos 
diferentes.


:::::::::::::::Common Actions:::::::::::::::
reduce(): Takes a function that operates on two elements of the type in your RDD and returns a new element of the same type.
RDD.reduce((x,y)=> x+y)

fold():Igual que el reduce pero provee un Zero-Value o valor identidad que se computará con la operación conjunta que estemos realizando eje 0 pra suma (0+1+2), 1 para * (1*1*2*3), lista vacía para concatenación de todos los elementos.

Un caso que encontramos con reduce and fold es el tema de que ambos resuelven al mismo tipo de dato de cada elemento del RDD eje  RDD[Int]  ----> Int
en el caso de la suma aplica rdd(1,2,3) = 6 peero hay casos en que lo que tenemos es algo así como Rdd[(a,b) (a b) (a b)] y queremos un promedip y resolver a Int Rdd[(a,b) (a b) (a b)] ----> Int...por lo cual lo que podemos hacer es un map que resuelva a  Rdd[(a b 1) (a b 1) (a b 1)] y ya podemos hacer reduce con los 1's

aggregate(): 
we supply an initial zero value of the type we want to return. We then supply a function to combine 
the elements from our RDD with the accumulator. Finally, we need to supply a second function to merge 
two accumulators, given that each node accumulates its own results locally.

val result = input.aggregate((0, 0))(
(acc, value) => (acc._1 + value, acc._2 + 1),
(acc1, acc2) => (acc1._1 + acc2._1, acc1._2 + acc2._2))
val avg = result._1 / result._2.toDouble


collect(): lO UTILIZAN mucho para pruebas ya que requiere que todo quepa en el driver context
take(n): lO UTILIZAN mucho para pruebas ya que puede resultar en un cuello de botella si se trata de grandes cantidades de datos en proceso 


top(): Para obtener los top elements , usando el ordenamieno por defecto de los datos (natural) o el que le personalicemos

takeSample(withReplacement, num, seed): take a sample of our data either with or without replacement.

foreach(): Ejecutar una acción sin que se retorna algo a driver context(una inserción en base de datos, enviar un JSOn a un servidor web)

count() 
countByValue()


:::::::::Converting Between RDD Types::::::::::::::::::::
mean()
variance()
Las dos anteriores aplican para RDDs de tipo numérico

join(): Para RDDs tipos Keys/Value

Para conversiones implicitas necesitamos el import 
import org.apache.spark.SparkContext._

::::::::PERSISTENCE()::::::::::::::::::::
usado para no hacer recomputos de un set de datos al que ya le hemos hecho algo pero que luego lo vamos a necesitar

- Lo que hace es algo así como hacer que los nodos guarden en memoria cada computo de la partición que le correspondió.
- De igual forma es tolerante a falloS en la manera en que si el nodo que había guardado x computo falla entonces spark vuelva a generar el 
computo perdido

-Se pueden replicar los datos en otros nodos para manejar el tema de fallo en un nodo x 

- Por defecto persist guarda los datos en el Heap de la JVM como objetos deserializados.
RECORDAR : 
Que en el Heap se guardan los omjetos en memoria y las clases , dura más porque va desde la ejecución del programa hasta su fin, peeero 
el del stack solo es en la ejecución del método, valor de la variable.
El garbage colector al encontrar Objects sin alguna referencia() lo borra
En el stack se guardan los valores de las variable, primitivos, EJECUCIÓN DE MÉTODOS, es más pequña porque usa el 
algoritmo de LIFO para liberar memoria, esto puede generar problemas de 

Heap memory is used by all the parts of the application whereas stack memory is used only by one thread of execution.

Whenever an object is created, it’s always stored in the Heap space and stack memory contains the reference to it. Stack memory only contains local primitive variables and reference variables to objects in heap space.
Objects stored in the heap are globally accessible whereas stack memory can’t be accessed by other threads.

Memory management in stack is done in LIFO manner whereas it’s more complex in Heap memory because it’s used globally. Heap memory is divided into Young-Generation, Old-Generation etc, more details at Java Garbage Collection.
Stack memory is short-lived whereas heap memory lives from the start till the end of application execution.
We can use -Xms and -Xmx JVM option to define the startup size and maximum size of heap memory. We can use -Xss to define the stack memory size.
When stack memory is full, Java runtime throws java.lang.StackOverFlowError whereas if heap memory is full, it throws java.lang.OutOfMemoryError: Java Heap Space error.
Stack memory size is very less when compared to Heap memory. Because of simplicity in memory allocation (LIFO), 
stack memory is very fast when compared to heap memory.



- example:
val result = input.map(x => x * x)
result.persist(StorageLevel.DISK_ONLY)
println(result.count())
println(result.collect().mkString(","))







::::::::::::::::::page 65 Working with Key/Value Pairs:::::::::::::::::::::::::::::::::::
Key/value RDDs are commonly
used to perform aggregations, and often we will do some initial ETL

Key/value  OR pairs RDDs
Example 4-2. Creating a pair RDD using the first word as the key in Scala
val pairs = lines.map(x => (x.split(" ")(0), x))


Since pair RDDs contain tuples, we need to pass functions that operate on tuples rather than on individual
elements. (see page 67 for transformation on Pairs RDDs)

PAGE67


::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::