Driver programs access Spark through a SparkContext object, which represents a
connection to a computing cluster. In the shell, a SparkContext is automatically


The nodes are called Executors.
:::::::::::::
Lo importante de diferenciar el Tranformation del Action es que el Transformation retorna otro RDD Y es una operación LAZY, es decir que no se ejecuta inmediatamente se defina y si en cadena varios LAzy operations y al final se APLICA 
un ACTION(no resuelve a RDD, it´s a result to the DRIVER PROGRAMor write data to an external storage system.) (EJEMPLO "first")entonces viene lo INTERESANTE, spark para porque encuentra el primero. Al no se Lazy los transformatio entonces imagine el desperdicio de memoria y el performance si por cada tranformer OP cargara en 
memoria el resultado.
:::::::::::::
Spark’s RDDs are by default recomputed each time you run an action on
them. If you would like to reuse an RDD in multiple actions, you can ask Spark to
persist it using RDD.persist()

OJO: para la nota anterios persist() es lo mismo que llamar cache()


::::::::::
parallelize() es otra forma de crear RDDs: Esta operacion para cuando uno está aprendiendo está bien pero en un ambiente productivo no es aconsejable porque va a requerir de deamsiada memoria para ello, PARA TEMAS DE TDD SI PUEDE APLICAR PORQUE NOS PRUEBAS DE UN SET DE REGISTROS PEQUEÑOS



::::::::::lineage graph::::::::::
Pensemos en este termino como el hecho de que Spark guarda registro o traza de las dependencias generadas entre los diferendes RDDs creados, 
ejemplo claro es el de hacer dos filter sobre un mismo RDD inicial y al final unirlos, cada filter generó un RDD nuevo pero Spark sabe de donde viene cada uno y como unirlos con UNION.

:::::::::::::::::
collect() shouldn’t be used on large datasets, BE sure that the amount of data is short or at last for your machine: EN la practica casi que no se usa por la misma razón


::::::::::::::::Passing Functions to Spark:::::::::::::::::::::::::::::

Muy importante saber esto ya que hay algo particularcon spark o más bien llamando a spark (mediante map, flatMap, filter....) que consiste en que 
si hacemos referencia a una funcion en la clase actual (y no mediate el uso de lambdas) usando o no la palabra this.xx , entonces se va a enviar a los
executors o nodos de spark toooodo el objeto completo que contiene la función y puede generar errores o problemas de comunicación una vez el objeto sea muy grande 

class SearchFunctions(val query: String) {
	def isMatch(s: String): Boolean = {
		s.contains(query)
	}
	def getMatchesFunctionReference(rdd: RDD[String]): RDD[String] = {
		// Problem: "isMatch" means "this.isMatch", so we pass all of "this"
		rdd.map(isMatch)
	}
	def getMatchesFieldReference(rdd: RDD[String]): RDD[String] = {
		// Problem: "query" means "this.query", so we pass all of "this"
		rdd.map(x => x.split(query))
	}
	def getMatchesNoReference(rdd: RDD[String]): RDD[String] = {
		// Safe: extract just the field we need into a local variable
		val query_ = this.query
		rdd.map(x => x.split(query_))
	}
}

en esos casos es mejor tener val locales en la función y pasarle a spark solo ése elemento para que el no interprete que se le pas todo el objeto.

:::::::::::::::Common Transformations:::::::::::::::
map:::: Recordar que arranca en una categoría y termina en la misma eje de RDD  ----> RDD
y puede ser de un tipo X a uno Y o tipos iguales  RDD[String]  ----> RDD[Boolean]    ......   RDD[String]  ----> RDD[String]  
Por ejemplo que de una URL saquemos el nombre de la compañía.

flatMap:::: Algunas veces vamos de RDD[String]  ----> RDD[Lis(String)]
val lines = sc.parallelize(List("hello world", "hi"))
val words = lines.flatMap(line => line.split(" "))
words.first() // returns "hello"

instead of ending up with an RDD of lists we have an RDD of the elements in those lists!!!!!!!!!!!!!!



::::::::::::matematical set operations:
union
distinct
intersection
subtract
cartesian(otherRDD):  
> cartesiado entr dos RDDs diferentes (pero de mismo tipo) Muy usado para identificar similitudes entre todos los posibles pares, como cada expectativa de un usuario en una oferta 
> O un cartesiano de un RDD con el mismo para ver similitudes entre sus registros
OJO PARA LA OPERACIONES ENTRE DOS RDDs ES necesarioq ue ambos sean del mismo tipo, es decir 2 RDDs de [String] [Boolean] etc peero no entre tipos 
diferentes.


:::::::::::::::Common Actions:::::::::::::::
reduce(): Takes a function that operates on two elements of the type in your RDD and returns a new element of the same type.
RDD.reduce((x,y)=> x+y)

fold():Igual que el reduce pero provee un Zero-Value o valor identidad que se computará con la operación conjunta que estemos realizando eje 0 pra suma (0+1+2), 1 para * (1*1*2*3), lista vacía para concatenación de todos los elementos.

Un caso que encontramos con reduce and fold es el tema de que ambos resuelven al mismo tipo de dato de cada elemento del RDD eje  RDD[Int]  ----> Int
en el caso de la suma aplica rdd(1,2,3) = 6 peero hay casos en que lo que tenemos es algo así como Rdd[(a,b) (a b) (a b)] y queremos un promedip y resolver a Int Rdd[(a,b) (a b) (a b)] ----> Int...por lo cual lo que podemos hacer es un map que resuelva a  Rdd[(a b 1) (a b 1) (a b 1)] y ya podemos hacer reduce con los 1's

aggregate(): 
we supply an initial zero value of the type we want to return. We then supply a function to combine 
the elements from our RDD with the accumulator. Finally, we need to supply a second function to merge 
two accumulators, given that each node accumulates its own results locally.

val result = input.aggregate((0, 0))(
(acc, value) => (acc._1 + value, acc._2 + 1),
(acc1, acc2) => (acc1._1 + acc2._1, acc1._2 + acc2._2))
val avg = result._1 / result._2.toDouble


collect(): lO UTILIZAN mucho para pruebas ya que requiere que todo quepa en el driver context
take(n): lO UTILIZAN mucho para pruebas ya que puede resultar en un cuello de botella si se trata de grandes cantidades de datos en proceso 


top(): Para obtener los top elements , usando el ordenamieno por defecto de los datos (natural) o el que le personalicemos

takeSample(withReplacement, num, seed): take a sample of our data either with or without replacement.

foreach(): Ejecutar una acción sin que se retorna algo a driver context(una inserción en base de datos, enviar un JSOn a un servidor web)

count() 
countByValue()


:::::::::Converting Between RDD Types::::::::::::::::::::
mean()
variance()
Las dos anteriores aplican para RDDs de tipo numérico

join(): Para RDDs tipos Keys/Value

Para conversiones implicitas necesitamos el import 
import org.apache.spark.SparkContext._

::::::::PERSISTENCE()::::::::::::::::::::
usado para no hacer recomputos de un set de datos al que ya le hemos hecho algo pero que luego lo vamos a necesitar

- Lo que hace es algo así como hacer que los nodos guarden en memoria cada computo de la partición que le correspondió.
- De igual forma es tolerante a falloS en la manera en que si el nodo que había guardado x computo falla entonces spark vuelva a generar el 
computo perdido

-Se pueden replicar los datos en otros nodos para manejar el tema de fallo en un nodo x 

- Por defecto persist guarda los datos en el Heap de la JVM como objetos deserializados.
RECORDAR : 
Que en el Heap se guardan los omjetos en memoria y las clases , dura más porque va desde la ejecución del programa hasta su fin, peeero 
el del stack solo es en la ejecución del método, valor de la variable.
El garbage colector al encontrar Objects sin alguna referencia() lo borra
En el stack se guardan los valores de las variable, primitivos, EJECUCIÓN DE MÉTODOS, es más pequña porque usa el 
algoritmo de LIFO para liberar memoria, esto puede generar problemas de 

Heap memory is used by all the parts of the application whereas stack memory is used only by one thread of execution.

Whenever an object is created, it’s always stored in the Heap space and stack memory contains the reference to it. Stack memory only contains local primitive variables and reference variables to objects in heap space.
Objects stored in the heap are globally accessible whereas stack memory can’t be accessed by other threads.

Memory management in stack is done in LIFO manner whereas it’s more complex in Heap memory because it’s used globally. Heap memory is divided into Young-Generation, Old-Generation etc, more details at Java Garbage Collection.
Stack memory is short-lived whereas heap memory lives from the start till the end of application execution.
We can use -Xms and -Xmx JVM option to define the startup size and maximum size of heap memory. We can use -Xss to define the stack memory size.
When stack memory is full, Java runtime throws java.lang.StackOverFlowError whereas if heap memory is full, it throws java.lang.OutOfMemoryError: Java Heap Space error.
Stack memory size is very less when compared to Heap memory. Because of simplicity in memory allocation (LIFO), 
stack memory is very fast when compared to heap memory.



- example:
val result = input.map(x => x * x)
result.persist(StorageLevel.DISK_ONLY)
println(result.count())
println(result.collect().mkString(","))







::::::::::::::::::page 65 Working with Key/Value Pairs:::::::::::::::::::::::::::::::::::
Key/value RDDs are commonly
used to perform aggregations, and often we will do some initial ETL

Key/value  OR pairs RDDs
Example 4-2. Creating a pair RDD using the first word as the key in Scala
val pairs = lines.map(x => (x.split(" ")(0), x))


Since pair RDDs contain tuples, we need to pass functions that operate on tuples rather than on individual
elements. (see page 67 for transformation on Pairs RDDs)

PAGE67


:::page 75:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

rdd.reduceByKey(func) produces the same RDD as rdd.groupBy
Key().mapValues(value => value.reduce(func)), seindo reduce más eficiente



Unas de las razones para no particionae es SI POR EJEMPLO Sí lel rdd is scanned only once entonces el particionamiento no aportará en nada 


 
:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

Data Partitioning (Advanced)
Se muestran las ventajas de .partitionBy(new HashPartitioner(100), make this at least as large
as the number of cores in your cluster) sobretodo en JOINs (ver pg 81 o 62  en el libro)
, en resumen es algo así como que en el momento de hacer jpin y distribuir los datos NO distribuye lo que ya se había distribuido antes sin control el OTHER rdd del join
lo anterior es importante por le tema de LATENCY 


:::::::::::::::::::::.CHAPTER 3 Loading and Saving Your Data ::::::::::::::::::::::::::::::::::::::::::::


PAGE 92
saveAsTextFile() : The path is treated as a directory and Spark will output multiple files underneath
that directory. 

page 103:
Se explican varios formatos de archivo desde los cuales podemos leer o escribir resultados, entre los mas reconocidos están:
textFiles
json 
csv
SequenceFiles
Hadoop inputs and outputs formats:
hadoopFile() and saveAsHadoopFile() family of functions, you
can use hadoopDataset / saveAsHadoopDataSet and newAPIHadoopDataset / saveAsNe


:::::Protocol buffers:::::::::::::::::::::::::::
Tips:
- When you’re adding new fields to existing protocol buffers it is good practice to make the new fields optional
- Es como un estilo formato json, REVISAR LAS BONDADES QUE OFRECE

::::::::File Compression::::::::::::::::::::::::
Spark ofrece la posibilidad de 

Limintante:
we normally try to read our data in
from multiple different machines. To make this possible, each worker needs to be
able to find the start of a new record

ver Table 5-3. Compression options, sobretodo los splitables 

::::::::::::::::::::::::::::::::::::::::::::::::

::::::::::::::Filesystems:::::::::::::::::::::::::::

Local/“Regular” FS: Requiere que los ARCHIvos se encuentren el el MISMO PATH en todos los workers del cluster, si no es así entonces que el file esté en el Driver y luego hace parallelize

Recomienda el autor:
putting your files in a shared filesystem like HDFS, NFS, or S3.

Example: val rdd = sc.textFile("file:///home/holden/happypandas.gz")


Amazon S3::::::
Se está volviendo famoso para almacenar grandes cantidades de datos.

HDFS:::::::::::::
Spark and HDFS can
be collocated on the same machines
-Brinda el tema de resilencia, al momento en que se caiga un nodo.

Structured Data with Spark SQL:::::::::::::::::::
Permite usar una sintaxis SQL para consulta de información en los archivos, el resultado en un RDD de objetos

Apache Hive::::::::::::::::
Es una fuente de datos esturcturados muy comun en Hadoop 
Hive can store tables in a variety of formats, from plain text to column-oriented formats,
Spark SQL can load any table supported by Hive

Podemos conectar Hive con Spark SQL (ver pg 91 real en el libreo o 109)

Eg

import org.apache.spark.sql.hive.HiveContext
val hiveCtx = new org.apache.spark.sql.hive.HiveContext(sc)
val rows = hiveCtx.sql("SELECT name, age FROM users")
val firstRow = rows.first()
println(firstRow.getString(0)) // Field 0 is the name

::::::::::::..JSON:::::::::::::::::::::::::
Sí tenemos archivos de datos Json y con una estructura consistente entonces podemos hacer cosas como :
Example 5-33. Sample tweets in JSON
{"user": {"name": "Holden", "location": "San Francisco"}, "text": "Nice day out today"}
{"user": {"name": "Matei", "location": "Berkeley"}, "text": "Even nicer here :)"}

val tweets = hiveCtx.jsonFile("tweets.json")
tweets.registerTempTable("tweets")
val results = hiveCtx.sql("SELECT user.name, text FROM tweets")



Databases::::::::::::.Spark se puede conectar a muuchas Bds, bien sea mediante los conectores de Hadoop o los de Spark como tal 


:::Java Database Connectivity (no estamos hablando de acceder programando en java, estamos hablando de JDBC)

def createConnection() = {
Class.forName("com.mysql.jdbc.Driver").newInstance();
DriverManager.getConnection("jdbc:mysql://localhost/test?user=holden");
}
def extractValues(r: ResultSet) = {
(r.getInt(1), r.getString(2))
}
val data = new JdbcRDD(sc,
createConnection, "SELECT * FROM panda WHERE ? <= id AND id <= ?",
lowerBound = 1, upperBound = 3, numPartitions = 2, mapRow = extractValues)
println(data.collect().toList)


::::::::::::Cassandra:::::::::::::::(PG94 y 95 REALes CONTIENE lo requerido para configurar con cassandra)
Spark’s Cassandra support has improved greatly
Se pueden cargar TABLAS enteras so sub set de datos tambien
::::::::::::::::::::::::::::::::::::::::::::::::::::
Example 5-44. Saving to Cassandra in Scala
val rdd = sc.parallelize(List(Seq("moremagic", 1)))
rdd.saveToCassandra("test" , "kv", SomeColumns("key", "value"))

This section only briefly introduced the Cassandra connector. For more information,
check out the connector’s GitHub page.


HBase::::::::::::::::::::::::::::::::::::::..
Spark can access HBase through its Hadoop input format, implemented in the
org.apache.hadoop.hbase.mapreduce.TableInputFormat class

:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
page 117::::::::.Advanced Spark Programming:::::::::::::::







