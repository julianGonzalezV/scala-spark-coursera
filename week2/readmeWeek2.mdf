::::::::::::::::::::::Distributed key/value pairs:::::::::::::::::::::::::::::::::::::


Así como en java por ejemplo usamos comunmente list o arrays, EN BIGA DATA LO MÁS COMUN ES 
KEY/VALUE PAIR y operar sobre estos o manipularlos es un punto clave

-Los dieñadores originales de Map-Reduce se enfocaron mucho en key - values 
-De las mejores estrategias a la hora de atacar problemas grandes y queder irlos bajandolos de categoría para poderlos tratar


-Formas de crear un Pair RDD?, antes recordar que spark tiene un trato muy especial con pair rdds por lo cual es el tipo de RDD que más nos vamos a encontrar
con map 




::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::


::2018-01-29::::TRANSFORMATIONS AND ACTIONS ON PAIR RDDs :::::::::::::::::::::::::::::

transformations
groupByKey:::::::::::: aplicando una funcion que relacione los elementos del RDD con un Key, 
Un jemeplo en scala collections es el de edades y hacer groupBy, diciendo que cierto rrango es joven, adulto, ninio
estamos diciendo ya que el Key es un String (joven, adulto, ninio) y cada Key representará un set de elementos de la colección scala que cumpleron la especificación de la función

En Spark es algo parecido pero lo que hace es agrupar toooodos los values que tienen el mismo key
OJO: Con el estilo de verificaciones como en el min 6:30 en donde se pregunta el resultado de groupByKey de un RDD de eventos, la respuesta era
QUE NO HACE NADA porque es un Operation OF trasformation  Y NO UN Action(como collect, countByKey)


reduceByKey::::::::::::can be thought as a combination of group by key and REDUCE , PERO es mas eficiente que usar ambos por separado!!
reduceByKey(fun(V,V)=> V):[RDD(K,V)]: Funcion que dada una funcion aplicada a los VALORES RESUELVE A  UN VALOR UNICO, al final retorna un listado de PAIR RDDs 

mapValues::::::::::::Aplica una funcion solo a los values de un Pair RDD

keys:::::::::::: def keys(RDD[k]): Retorna un RDD con los Key de cada tupla, ES UN TRANSFORMATION POR EL HECHO DE QUE PUEDE ESTAR TRABAJANDO CON DEMASIADOS REGISTROS O CANTIDAD DE DATOS
E IMAGINemos el problema de espacio en memoria si fuera un Action :S

Joins: Es de las operaciones MAS COMUNES EN PAIR RDDS 
hAY DOS TIPO 
INNERjoins: pARA key en ambos RDDs

Outer Joins: Para Keys que solo están en uno de los dos (leftOuterJoin, RightOuterJoin)

join(other:RDD)= retorna un Rdd con los elelemntos que contienen el mismo key retunr RDD[(k,(v,W))  v es el value del primer RDD y w el value del segundo RDD , obviamente k es el Key que debe ser igual 
en ambos RDD, OJO AMBOS RDDS DEBEN TENER KEYS DEL MISMO TIPO 

leftOuterJoin: eje A.leftOuterJoin(B) RETORNA TOOODOS los keys de A que no no necesariamente deban estar en B (option(w)), sí no están en B el resultado sería algo así 
como (k,(v,None)) pero si hay valor entonces (k,(v,Some(w)))
RightOuterJoin: Lo contario de leftOuterJoin: (k,(None,w)) pero si hay valor entonces (k,(Some(v),w))

OJO EN VARIAS OCACIONES v ó w pueden ser pairs (x,y) y hace la estructrua un poco larga y difícil de leer
Algo que también es claro anotar es que muchas veces hay redundancia en los resultados, debido a que un Key está en varias veces en un Rdd y una sola vez en el otro RDD

Action

countByKey:::::::::::::::::::.Counts the number of elements per Key , returning a normal scala Map

Hacer el ejercicio de BUDGET PER ORGANIZER 






Notas que iban dentro de la función @tailrec final def kmeans(...
ahora las coloco acá para que no se vea tan sucion el código 

:::::::::::::::::::::::::::::::::::::::INICIO:::::::::::::::::::::::::::::::
/*System.out.println("----------MEANS----------")
    means.foreach(println(_))
    System.out.println("-------------------------")

    System.out.println("----------vectors----------")
    vectors.foreach(println(_))
    System.out.println("-------------------------")
*/





    /*
    Ojo en la asignación nos piden calcular el newMeans por cada iteración,  ya que estamos dentro de una
    funcion recursiva inicialmente nos dan :
    val newMeans = means.clone() // you need to compute newMeans
    PERO LO DEBEMOS IMPLEMENTAR PORQUE SINO ENTONCES VA ENTRAR EN EL LOOP INFINITO, CLARO QUE EN
    LA LÍNEA  214V IF iter < kmeansMaxIterations  SE CONTRARRESTA ESTO PERO IGUAL HARÍA DEMASIADAS iTERACIONES
     */


    /*
    Cómo calcularía el newMeans?
    - means (input) es un vector con valores/puntos iniciales (ver definicion de sampleVectors) aleatorios que sirve como punto de partida
    para sacar la diatancia euclidiana entre means y newMeans e ir afinando mean, que no es mas que un cluster con registros asociados

    -vectors son PAIRS RDDs donde la primera posición representa el Indice del lenguake multiplicado por langSpread
    y la segunda posición la respuesta a ése Question con mayor score o votacion

    El proceso a seguir es:

     1- Pick k points called means. This is called initialization.

     2- Associate each input point with the mean that is closest to it.
      We obtain k clusters of points, and we refer to this process as
      classifying the points.

     3-  Update each mean to have the average value of the corresponding cluster.

     4- If the k means have significantly changed, go back to step 2.
      If they did not, we say that the algorithm converged.

     5- The k means represent different clusters -- every point is in the cluster
      corresponding to the closest mean.


     */
    /*
    Acá estamos aplicando el paso 2 classifiedLangs es un pair rdd RDD[(MeansIndex, (LangIndex, HighScore))]
     que contiene en su primer elemento el index en means que da  la menor distancia
     Y EN su segundo elemento una tupla del tipo (LangIndex, HighScore)
     el punto que se resuelve aca es el de la semana 2 :
     pairing each vector with the index of the closest mean (its cluster)

     */


/*

    System.out.println("newMeans2.length "+newMeans2.length + "  newMeans.length "+newMeans.length)
    System.out.println("newMeans2 es::::::::::::::::::::::::::::::::: ")
    System.out.println(newMeans2.foreach(print(_)) )
    System.out.println("newMeans es::::::::::::::::::::::::::::::::: ")
    System.out.println(newMeans.foreach(print(_))  )

    //val v1 = newMeans.updated(0,(1,2))
    val vt = Array((0,7),(0,6))
*/


/*
    val vt2 = for (i <- 0 to (vt.size-1)) yield{
      val  items = newMeans.updated(i,vt(i))
      items
    }*/
/*

    System.out.println("nuevo newMeans es::::::::::::::::::::::::::::::::: ")
    System.out.println(newMeans.foreach(print(_))  )
*/

    //val newMeans = classifiedLangsByCluster.mapValues(averageVectors).map(x=>x._2).collect()

    /*
    System.out.println("-------------newMeans-------")
    newMeans.foreach(println(_))*/
:::::::::::::::::::::::::::::::::::::::FIN:::::::::::::::::::::::::::::::


::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
