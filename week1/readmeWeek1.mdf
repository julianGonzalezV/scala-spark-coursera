Scaling computations based on funcional paradingm(including frameworks like spark) is eaier than scaling over an imperative one

What Spark is?
Spark is a functionally oriented framework for large scale data processing that's implemented in Scala. 

Why not  R, Python, Octave and MATLAB:
Because they are good for small amount of info, and this languague or frameworks wont allow us to scale in an easy way, we have to re-implement a lot.

- Imaginemos el poder recolectar toda la onformación de los smart phones de nuestros clientes y saber por donde transita, que actividades hace, que ropa usa al realizar actividades
especificas y demás esto es algo que las empresas podrían sacar demasiado provecho e impulsar sus negocios.

-El uso de scala es porque vamos a poder scalar nuestro problemas pequeños a más grande con facilidad, esto es, si antes ejecutabamos nuestro 
computo en un solo nodo, facilmente lo podemos correr en muuuchos más, sin tener que reescribir el código o cosas drásticas. ADEMÁS DE QUE SI SE PUEDE CON OTROS
EL COSTO EN TIEMPO Y PRODUCTIVIDAD DE DESARROLLO, SUMANDOLE QUE EL DEVELOPER DEBE TENER UNA EXPERIENCIA ENORME, SPARK AYUDA MUCHO EN ESO PORQUE LA FORMA EL DIEÑO , ETC YA ESTÁ


 REASONS why we should use Spark

 1- Spark es más expresivo:  Nos brinda Apis para el manejo de computaciones distribuidos en una forma que se parece a las listas inmutables de scala, 
dando mas operciones que se pueden componer(HOF, map, flatMap, filter, reduce ) y no solo ofreciendo la posibilidad de MapReduce como lo hace Hadoop 
que de forma rigida define solo el map and reduce sin las combinaciones o las composiciones anteriores (HOF, map, flatMap, filter, reduce )

 2- Performance no  solo en tiempo del computo sino en la interacción con los developers (mejorando la productividad del developer) CONSIDERADOS COMO DATA ANALYST (NUEVO ROL EN LAS COMPAÑIAS)

 3- Good for DATA Sciense: It enables Iteration, that is required by most  algorithms in data Sciense's toolbox, that means that multiple passes in de set of date are required by  data Sciense algortihms.
	En hadoop se puede hacer pero con ciertas librerías que lo que hacen es cambiar lo que se tiene , mientras que en spark no.



::::::::::::::::::::::::::::DATA PARALLEL TO ---- DISTRIBUTED DATA PARALLEL ::::::::::::::::::::::::::::

 Cómo es el procesamiento de datos en Paralelo(data parallel)?

Imaginemos un solo jar-tarro(lleno de dulces de colores) y vamos a ejecutar una accion en cada  uno de los elementos del tarro (map), imagine que es cambiar el color etc
-Lo que hace la prog paralela es aprovecha la cantidad de cores(núcleos) y ejecuta la operacion.
 1- -Divide el set de datos - PARTICIONAMIENTO EN MEMORIA (CREO QUE LAS BDS LO HACEN CUANDO SE EJECUTA UN PARALLEL)
 2- Establecer workers o hilos que van a procesar cada pedazo de datos resultante del punto 1
 3- hace Join o combine de todos los procesos independientes para dar el resultado final  (solo si un resultado final es requerido, muchas veces el resultad es la operacion ejecutada en el paso 2 y no un unico resultado)
Todo lo anterior usando una memoria compartida (shared memory) y un solo nodo or ON THE SAME MACHINE!!


DE QUE SE TRATA EL PROCESAMIENTO DE DATOS EN PARALELO DISTRIBUIDO ?
- YA NO hay una memoria compartida 
1- Divide el set de datos EN DIFERENTES NODOS. el jar ya se vuelte en un COLECCIÓN DISTRIBUIDA
2- Los NODOS independiente mente operan sobre esos pedazos de código.
3- hace Join o combine de todos los procesos independientes para dar el resultado final  (solo si un resultado final es requerido, muchas veces el resultad es la operacion ejecutada en el paso 2 y no un unico resultado)



	
A todo lo anterior se le suma un nuevo reto a enfrentar ...la Latencia!!

para e computo paralelo distribuido se usa Apache Spark

Spark implementa un modelo de Distributed Data Parallel llamado RDDs, Resilient(elástico, eje que permite extenderse on contraterse ante eventos com mayor o menor carga) Distributed Data Set
	
En donde el set a atacar se trata como una coleccion(listado) y spark aplica un algoritmo de PARTICIONAMIENTO y se inicia con la operacion sobre el listado, nosotros solo hacemos Lisxx.map y spark solo 
ya sabe que debe particionar y ejecutar la operación, fatástico no?? 

::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

:::::::::::::::::::Latency:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

Cuando nos vamos al mundo de DISTRIBUTED DATA PARALLEL, surgen las siguientes preocupaciones:

Partial Failure: Fallas de un subconjunto de maquinas que intervienen en el computo distribuido, esto no sucede en shared memory(data parallel). POR LO cual ESTE nos obliga a pensar 
que hacer sí un nodo está caido o presenta fallas???

Latency: Los nodos en el computo distribuido responde en tiempos distintos debido a la conexión de red, la latencia que suma la red. POR LO CUAL este afecta la velocidad en el computo como tal



IMPORTANT LATENCY NUMBER:

Tabla que nos indica el tiempo que requiere X operacion en MEMORIA, DISCO O EN LA RED
EL resutado se puede resumir en:
	Operaciones en Memoria son más rápidas
	Operaciones en Disco son lentas 
	Operaciones en la Red son las MAS LENTAS



Hadoop SE CONSIDERA EL predecesor de Spark, Es un framework de procesamiento de datos a gran escala, es una implementación Open Source del algoritmo MapReduce de Google.

Y lo que hadoop ofrecia era porder escalar computos que se hacian en decenas o centenaas de nodos a miles de nodos y mucho más. Claro la probabilidad de un nodo fallando es mucho MAYOR
PEEERO ahí es cuando viene lo bacano, hadoop sabe ejecutar el computo y asgura al programador que se va a ejecutar, y si existe un nodo caído el sabe como re-computar el set de datos que
allí se estaba procesando.

entonces brinda Fault-Tolerance + Simple API.

ENTONCES POR QUE NOS BASAMOS EN SPARK??
Debido a que Hadoop con el fin de tratar con el (deal with) problema de fault tolerance escribe las operaciones intermedias en disco(para recordar en que iba en caso de falla) y como vimos en IMPORTANT LATENCY NUMBER, Operaciones en Disco son lentas 
y si le suma el de la red QUE ES ALGO CON LO QUE DEBEMOS SER CONSCIENTES DE QUE VA A EXISTIR entonces vienen los problemas de LATENCIA EN CADA NODO


SPARK YA SABE COMO ENFRENTAR EL FAULT TOLERANCE Y ADEMÁS se basa en PROGRAMACIÓN FUNCIONAL PARA LOS PROBLEMAS DE LATENCIA Y SE DESHACE DE TENER QUE ESCRIBIR DEMASIADO EN DISCO y mas en Memoria

Cómo ?
Trata de que la mayoría de los datos que requiere estén todos en memoria e inmutables, esto permite que al hacer una transformacion de los datos (map filter etc) se retorne otro set de datos inmutable
como en scala collections y de esta manera si un nodo falló en uno de los calculos entonces Spark lo que hace es recordar en que transformación iba el computo y le hace replay en ése mismo nodo o en otro 

EL hecho de hacer el mayor trabajo en memoria hace que sea 100x más veloz que Haddop (ver tabla IMPORTANT LATENCY NUMBER para saber el motivo)
De hecho hay estadisticas que muestra cómo desde la primera iteración sobre un set de datos Spark supera a Hadoop y al volver a correr o iterar es mucho más rápido, por lo de la info en caché

::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::


RDDS:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
Resilient Distributed Datasets: Spark's Distributed Collections AbstrActions and they're really are the core of spark
Muy similares a Colecciones inmutables




Existen 2 formas de crear RDDs: 
1- A partir de la transformación de otro RDD (mediante map, filter, etc).
2- A partir de la creación mediante sparkContext or SparkSession:	
	parallelize: convierte una lista scala en un RDD

	textFile: lee el contenido de un archivo desde HDFs(hadoop file system)
	o a partir del sistema local de archivos y crea un RDD de String donde cada línea del archivo es un elemento de la colección
	siendo text file el mas usado porque la mayoría de veces nos van a pasar un set de datos a nalizar en un sistema de archivos

 
RDDs: Transformations and Actions:

En scala se tienen:
Transformers: Retornan una nueva lista como resultado map, filters, flatMap, groupBy
Accessor: retornan un valor reducido eg reduce, fold, agregate


Llevandolo a Spark tenemos 
- Transformations: Retornan un nuevo RDD. Transformation son operaciones tipo LAZY , donde el resultado no se computa de manera inmediata sino que se resuelve cuando es requerida o cuando se 
   llama


- Actions: actua sobre un RDD y retorna un resultado o lo guarda el computo en un sistema de almacenamiento externo ejemplo HDFS , nO RTORNA UN RDD sino algo que es un valor o resultado 
	Actions son operaciones tipo EAGER, donde el resultado se computa inmediatamente


Porque es importante lo de Lazy: Debido a que se trata de que existe la minima interacción con la RED (latency drawback)

Y como se utiliza en la vida real, por lo que he visto el transformation define y el action ejecuta esa definiciṕn 

entonces un error que cometemos es pensar que algo pasa con operaciones de Trasnformación y que si aplicamos map, flamMat, Filter..entonces algo pasa con el CLUSTER cuando NOO es así sino hasta que se llama una operacion tipo ACTION , por ejemplo un reduce.

 Common Actions: reduce (unico valor u otro tipo pero no un RDD), fold, count foreach(apply a function to RDD pero es action type porque el resultado en un UNIT y no un RDD)
collect retorna un Array[A , takeretorna un Array[A , -----------------------EAGER!!!


 Common Transformations: Map, flatMap, filter,Distinct, ------------------------LAZY!!!  deferred que le llaman 
porque se ejecuta cuando un action lo requiere

LO BACANO DE ESO ES EL STREAMING : Note como cada elemento se le va a aplicar todas las transformaciones y va saliendo al final como resultado uno a uno
u no primero filter a todo, luego map a todo, lueg xx , tomaría demasiado tiempo




::::::::::::::::: Transformations ON two RDDs ::::::::::::::::: Por que se llama Transformation?? porque todas retornan un tipo RDD
Union: Rdd con l contenido de ambos RDDs
Intersection: Solo los iguales de ambos
Substrat : Los de A que no están en B
Cartesian : Producto cartesaion entre el RDD A y el RDD B 




OTHER USEFUL RDD ACTIONS:
TakeSample returns an Array[T   Para reducir el tamanio del dataSet 
TakeOrdered returns an Array[T  : Para reducir el tamanio del dataSet , hacer un take de x elementos peeero que el resultado se encuentre ordenado, natural o un comparador personalizado
SaveAsTextFile returns Unit :  
SaveAsSequenceFile returns Unit  :   




:::::Evaluation in Spark: Diferente(unlike) a scala collections:::::::::::::::::

Cada Iteración en Hadoop escribe en disco de nuevo y la siguiente iteracion debe volver a leer del disco :S
COn Spark no pasa esto sin que de una se procede con al siguiente iteraciṕn porque todo está en memoria.



RDDS SE VUELVEN A COMPUTAR O RESOLVER O CALCULAR CADA VEZ QUE UNA OPERACION TIPO aCTION ES EJECUTADA!!!!
Para evitar lo anterior entonces guardemos en memoria los computos realizados , en spark se logra haciendo uso de la funcion persist(), para persistir el RDDs
algo así como que le l alínea 10 guardemos el val de un filter transformation y ese val en la línea 11 le apliquemos un Actions operation y luego en la línea 12 otro
 Action operation, sí no existe el PERSIS() al final entonces la referencia en la línea 10 vuelve y se comuta, es decir que vuelve y ejecuta el filter.

Lo anterior es SUPER IMPORTANTE porque si lo aplciamos vamos a mejorar el tiempo de respuesta DEBEMOS APLICARLO A NUESTRO TRABAJO DIARIO Y NO OLVIDAR ESTE TIP


OTRO TIP ES USAR LAS OPEraciones tipo Transformation juntas o seguidas una después de otra , es como si en un for tengamos todos los metodos transformadores y no voler a hacer otro 
for para la siguiente función transformadora




:::::::::::::::::CLUSTER TOPOLOGY MATTERS!!










